{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Besitzer\\Downloads\\dev\\gptzero\\gpt0\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os , re, openai\n",
    "import sys\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from utils import rephrase_essay, compute_similarity, mask_sentences, assess_quality,\\\n",
    "    generate_replacements_for_masked_sentences, substitute_word, read_essay_from_file,\\\n",
    "    evaluate_detection_evasion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Besitzer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Besitzer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Besitzer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize models with a valid API key\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code cell is for text manipulation. \n",
    "\n",
    "student_essay_path = \"human\"\n",
    "gpt_essay_path = \"gpt_essays\"\n",
    "manipulated_essay_path = \"manipulated_essays\"\n",
    "\n",
    "# Ensure the manipulated_essay_path directory exists\n",
    "if not os.path.exists(manipulated_essay_path):\n",
    "    os.makedirs(manipulated_essay_path)\n",
    "\n",
    "# Listing files from both directories\n",
    "student_essay_files = sorted([os.path.join(student_essay_path, f) for f in os.listdir(student_essay_path) if os.path.isfile(os.path.join(student_essay_path, f))])\n",
    "gpt_essay_files = sorted([os.path.join(gpt_essay_path, f) for f in os.listdir(gpt_essay_path) if os.path.isfile(os.path.join(gpt_essay_path, f))])\n",
    "\n",
    "\n",
    "def manipulate_gpt_essay():\n",
    "    max_essays = 3\n",
    "    student_essay_files = student_essay_files[:max_essays]\n",
    "    gpt_essay_files = gpt_essay_files[:max_essays]\n",
    "    # Process each pair of essays to apply the desired manipulations\n",
    "    for student_file, gpt_file in zip(student_essay_files, gpt_essay_files):\n",
    "        # Read essays from files\n",
    "        print(student_file)\n",
    "        print(gpt_file)\n",
    "        student_essay = read_essay_from_file(student_file)\n",
    "        gpt_essay = read_essay_from_file(gpt_file)\n",
    "\n",
    "        # Rephrase the essay \n",
    "        rephrased_essay = rephrase_essay(student_essay, gpt_essay)\n",
    "        \n",
    "        # Step 3: Apply sentence-level replacement to the rephrased essay\n",
    "        masked_essay, sentences_to_replace = mask_sentences(rephrased_essay, int(len(sent_tokenize(rephrased_essay)) * 0.5))\n",
    "        sentence_substituted_essay = generate_replacements_for_masked_sentences(masked_essay, sentences_to_replace)\n",
    "        \n",
    "        # Step 4: Apply word-level replacement\n",
    "        word_substituted_essay = substitute_word(sentence_substituted_essay, 52)\n",
    "\n",
    "        # Saving the word substituted essay to the specified directory\n",
    "        output_file_path = os.path.join(manipulated_essay_path, os.path.basename(gpt_file))\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(word_substituted_essay)\n",
    "\n",
    "# manipulate_gpt_essay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Essays: ['1.txt', '10.txt', '100.txt', '101.txt', '102.txt', '103.txt', '104.txt', '105.txt', '106.txt', '107.txt', '108.txt', '109.txt', '11.txt', '110.txt', '111.txt', '112.txt', '113.txt', '114.txt', '115.txt', '116.txt']\n",
      "GPT Essays: ['1.txt', '10.txt', '100.txt', '101.txt', '102.txt', '103.txt', '104.txt', '105.txt', '106.txt', '107.txt', '108.txt', '109.txt', '11.txt', '110.txt', '111.txt', '112.txt', '113.txt', '114.txt', '115.txt', '116.txt']\n",
      "Manipulated Essays: ['1.txt', '10.txt', '100.txt', '101.txt', '102.txt', '103.txt', '104.txt', '105.txt', '106.txt', '107.txt', '108.txt', '109.txt', '11.txt', '110.txt', '111.txt', '112.txt', '113.txt', '114.txt', '115.txt', '116.txt']\n"
     ]
    }
   ],
   "source": [
    "student_essay_path = \"human\"\n",
    "gpt_essay_path = \"gpt_essays\"\n",
    "manipulated_essay_path = \"manipulated_essays\"\n",
    "\n",
    "# Get the list of manipulated essay filenames\n",
    "manipulated_essay_filenames = sorted([f for f in os.listdir(manipulated_essay_path)])\n",
    "\n",
    "# Filter the student and GPT essays to include only those present in manipulated essays\n",
    "student_essay_files = sorted([os.path.join(student_essay_path, f) for f in os.listdir(student_essay_path) if f in manipulated_essay_filenames])\n",
    "gpt_essay_files = sorted([os.path.join(gpt_essay_path, f) for f in os.listdir(gpt_essay_path) if f in manipulated_essay_filenames])\n",
    "\n",
    "# Now, adjust the max_essays based on the filtered list\n",
    "max_essays = min(20, len(manipulated_essay_filenames), len(gpt_essay_files))\n",
    "\n",
    "student_essay_files = student_essay_files[:max_essays]\n",
    "gpt_essay_files = gpt_essay_files[:max_essays]\n",
    "manipulated_essay_files = [os.path.join(manipulated_essay_path, f) for f in manipulated_essay_filenames[:max_essays]]\n",
    "\n",
    "# Verify the corresponding filenames\n",
    "print(\"Student Essays:\", [os.path.basename(f) for f in student_essay_files])\n",
    "print(\"GPT Essays:\", [os.path.basename(f) for f in gpt_essay_files])\n",
    "print(\"Manipulated Essays:\", [os.path.basename(f) for f in manipulated_essay_files])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling GPT-4 API for rephrasing...\n",
      "Score: 9\n",
      "Score: 8\n",
      "True: [1, 0]\n",
      "Predicted: [1, 0]\n",
      "Predicted probabilities: [0.9977775103937533, 0.17406313434379872]\n",
      "True: [1, 0]\n",
      "Predicted: [1, 0]\n",
      "Predicted probabilities: [0.9977775103937533, 0.17406313434379872]\n",
      "True: [1, 0]\n",
      "Predicted: [1, 0]\n",
      "Predicted probabilities: [0.9977558223168834, 0.17406313434379872]\n",
      "True: [1, 0]\n",
      "Predicted: [1, 0]\n",
      "Predicted probabilities: [1, 0.17406313434379872]\n",
      "Calling GPT-4 API for rephrasing...\n",
      "Score: 9\n",
      "Score: 8\n",
      "True: [1, 0]\n",
      "Predicted: [1, 0]\n",
      "Predicted probabilities: [0.9977558223168834, 0.0027005180975096872]\n",
      "True: [1, 0]\n",
      "Predicted: [1, 0]\n",
      "Predicted probabilities: [0.9977558223168834, 0.0027005180975096872]\n",
      "True: [1, 0]\n",
      "Predicted: [1, 0]\n",
      "Predicted probabilities: [0.982932157729323, 0.0027005180975096872]\n",
      "True: [1, 0]\n",
      "Predicted: [1, 0]\n",
      "Predicted probabilities: [0.9977558223168834, 0.0027005180975096872]\n",
      "Calling GPT-4 API for rephrasing...\n"
     ]
    }
   ],
   "source": [
    "# Data container for the results\n",
    "similarity_quality_results = []\n",
    "\n",
    "# Process each pair of essays\n",
    "for i in range(max_essays):\n",
    "    # Read essays from lists above\n",
    "     # Read essays from lists above\n",
    "    student_essay = read_essay_from_file(student_essay_files[i])\n",
    "    gpt_essay = read_essay_from_file(gpt_essay_files[i])\n",
    "    word_substituted_essay = read_essay_from_file(manipulated_essay_files[i])\n",
    "    rephrased_essay = rephrase_essay(student_essay, gpt_essay)    \n",
    "        \n",
    "        # Calculate Simhash similarity\n",
    "    similarity_before_word_substitution = compute_similarity(student_essay, rephrased_essay)\n",
    "    similarity_after_word_substitution = compute_similarity(student_essay, word_substituted_essay)\n",
    "\n",
    "    # Compute essay quality before and after manipulations\n",
    "    quality_score_before = assess_quality(rephrased_essay)\n",
    "    quality_score_after = assess_quality(word_substituted_essay)\n",
    "\n",
    "    # Evaluate GPT Essay\n",
    "    gpt_detection_metrics = evaluate_detection_evasion(gpt_essay, student_essay)\n",
    "\n",
    "    # Evaluate GPT Rephrased Essay \n",
    "    rep_detection_metrics = evaluate_detection_evasion(rephrased_essay, student_essay)\n",
    "\n",
    "    # Evaluate Sentence Substituted Essay\n",
    "    masked_essay, sentences_to_replace = mask_sentences(rephrased_essay, int(len(sent_tokenize(rephrased_essay)) * 0.75))\n",
    "    sentence_substituted_essay = generate_replacements_for_masked_sentences(masked_essay, sentences_to_replace)\n",
    "    ss_detection_metrics = evaluate_detection_evasion(sentence_substituted_essay, student_essay)\n",
    "\n",
    "    # Evaluate Word Substituted Essay\n",
    "    ws_detection_metrics = evaluate_detection_evasion(word_substituted_essay, student_essay)\n",
    "\n",
    "    # Add results for this essay pair to the list\n",
    "    similarity_quality_results.append([\n",
    "        os.path.basename(student_essay),\n",
    "        similarity_before_word_substitution,\n",
    "        similarity_after_word_substitution,\n",
    "        quality_score_before / 10,\n",
    "        quality_score_after / 10,\n",
    "        gpt_detection_metrics['ACC'], gpt_detection_metrics['AUROC'], gpt_detection_metrics['F1'],\n",
    "        rep_detection_metrics['ACC'], rep_detection_metrics['AUROC'], rep_detection_metrics['F1'],\n",
    "        ss_detection_metrics['ACC'], ss_detection_metrics['AUROC'], ss_detection_metrics['F1'],\n",
    "        ws_detection_metrics['ACC'], ws_detection_metrics['AUROC'], ws_detection_metrics['F1']\n",
    "    ])\n",
    "\n",
    "# Define DataFrame columns\n",
    "columns = [\n",
    "    'Essay', \n",
    "    'Simhash Before', 'Simhash After', \n",
    "    'Quality Before', 'Quality After', \n",
    "    'GPT ACC', 'GPT AUROC', 'GPT F1', \n",
    "    'Rephrased ACC', 'Rephrased AUROC', 'Rephrased F1', \n",
    "    'Sentence Sub ACC', 'Sentence Sub AUROC', 'Sentence Sub F1', \n",
    "    'Word Sub ACC', 'Word Sub AUROC', 'Word Sub F1'\n",
    "]\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "similarity_df = pd.DataFrame(similarity_quality_results, columns=columns)\n",
    "similarity_df.to_csv(\"manipulation_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
